-- Flink SQL Job: Vulnerability Detection via Temporal Join
--
-- This job performs a bi-directional join between:
--   1. sbom_events (streaming SBOMs from container scans)
--   2. cve_feed (compacted CVE database)
--
-- Output: vulnerability_matches when a package in an SBOM matches an active CVE

-- ============================================================================
-- SOURCE: SBOM Events Stream
-- ============================================================================
CREATE TABLE sbom_events (
    image_id STRING,
    image_digest STRING,
    scan_timestamp STRING,  -- ISO8601 timestamp
    packages ARRAY<ROW<
        purl STRING,
        name STRING,
        version STRING
    >>,
    event_time AS TO_TIMESTAMP(scan_timestamp),
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'sbom_events',
    'properties.bootstrap.servers' = 'kafka:29092',
    'properties.group.id' = 'flink-sbom-consumer',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'json',
    'json.fail-on-missing-field' = 'false',
    'json.ignore-parse-errors' = 'true'
);

-- ============================================================================
-- SOURCE: CVE Feed (Compacted Topic as Changelog)
-- ============================================================================
CREATE TABLE cve_feed (
    package_purl STRING,
    cve_id STRING,
    affected_versions STRING,
    severity STRING,
    status STRING,
    description STRING,
    cvss_score DOUBLE,
    published_at STRING,
    PRIMARY KEY (package_purl) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'cve_feed',
    'properties.bootstrap.servers' = 'kafka:29092',
    'properties.group.id' = 'flink-cve-consumer',
    'key.format' = 'raw',
    'value.format' = 'json',
    'value.json.fail-on-missing-field' = 'false',
    'value.json.ignore-parse-errors' = 'true'
);

-- ============================================================================
-- SINK: Vulnerability Matches (Compacted Output Topic)
-- ============================================================================
CREATE TABLE vulnerability_matches_kafka (
    match_key STRING,  -- image_id:cve_id
    image_id STRING,
    cve_id STRING,
    package_purl STRING,
    severity STRING,
    detected_at TIMESTAMP(3),
    status STRING,
    PRIMARY KEY (match_key) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'vulnerability_matches',
    'properties.bootstrap.servers' = 'kafka:29092',
    'key.format' = 'raw',
    'value.format' = 'json'
);

-- ============================================================================
-- SINK: PostgreSQL (Materialized View for Grafana)
-- ============================================================================
CREATE TABLE vulnerability_matches_pg (
    image_id STRING,
    cve_id STRING,
    package_purl STRING,
    severity STRING,
    detected_at TIMESTAMP(3),
    status STRING,
    PRIMARY KEY (image_id, cve_id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://postgres:5432/sca_demo',
    'table-name' = 'vulnerability_matches',
    'username' = 'sca',
    'password' = 'sca_password',
    'driver' = 'org.postgresql.Driver'
);

-- ============================================================================
-- PROCESSING: Temporal Join - SBOM packages âŸ• CVE feed
-- ============================================================================
-- This join fires when:
--   1. A new SBOM arrives with a package matching an existing active CVE
--   2. A new CVE arrives matching packages in cached SBOMs (via Flink state)

CREATE TEMPORARY VIEW vulnerability_detections AS
SELECT
    s.image_id,
    c.cve_id,
    p.purl AS package_purl,
    c.severity,
    s.event_time AS detected_at,
    c.status
FROM sbom_events s
CROSS JOIN UNNEST(s.packages) AS p(purl, name, version)
JOIN cve_feed FOR SYSTEM_TIME AS OF s.event_time AS c
    ON REGEXP_EXTRACT(p.purl, '^([^@]+)', 1) = c.package_purl
WHERE c.status = 'active';

-- ============================================================================
-- OUTPUT: Write to both Kafka and PostgreSQL
-- ============================================================================

-- Write to Kafka (source of truth)
INSERT INTO vulnerability_matches_kafka
SELECT
    CONCAT(image_id, ':', cve_id) AS match_key,
    image_id,
    cve_id,
    package_purl,
    severity,
    detected_at,
    status
FROM vulnerability_detections;

-- Write to PostgreSQL (materialized view for queries)
INSERT INTO vulnerability_matches_pg
SELECT
    image_id,
    cve_id,
    package_purl,
    severity,
    detected_at,
    status
FROM vulnerability_detections;
